{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0aa7c017",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "from extract import POLOCM2\n",
    "from utils import read_json_file, GeneralTimeOut, set_timer_throw_exc, read_plan\n",
    "from collections import defaultdict\n",
    "from evaluator import ExecutabilityEvaluator\n",
    "import numpy as np\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a932fa85",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIR = '../../output/p2'\n",
    "OUTPUT_DIR = '../../output/p2_test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "54776614",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_single_experiment(cplex_dir,cplex_threads, extraction_type, learning_obj, dod , test_data, invalid_test_suffixes, logger):\n",
    "    \"\"\"Runs a single experiment given the necessary parameters.\"\"\"\n",
    "    domain = learning_obj['domain']\n",
    "    traces = learning_obj['traces']\n",
    "    all_po_traces = learning_obj['po_traces']\n",
    "\n",
    "    logger.info(f\"Running {domain}-lo.{learning_obj['id']}-{dod} ...\")\n",
    "\n",
    "    extractions = {\n",
    "        'p2': POLOCM2,\n",
    "    }\n",
    "    extraction = extractions[extraction_type]\n",
    "\n",
    "    \n",
    "    try:\n",
    "        index_by_dod = int(dod* 10 -1)\n",
    "        po_traces = all_po_traces[index_by_dod]\n",
    "        actual_dod = sum([poat.flex for poat in po_traces]) / len(po_traces)\n",
    "        runtime, accuracy_val, error_rate,acceptance_rate , invalid_acceptance_rate, remark = solve(\n",
    "            cplex_dir,\n",
    "            cplex_threads, \n",
    "            extraction,\n",
    "            po_traces,\n",
    "            traces,\n",
    "            domain,\n",
    "            test_data, \n",
    "            invalid_test_suffixes,\n",
    "        )\n",
    "\n",
    "\n",
    "    except GeneralTimeOut as t:\n",
    "        runtime, accuracy_val, error_rate,acceptance_rate , invalid_acceptance_rate, remark = (600,0,0), 0,0,0, 0, f\"Timeout\"\n",
    "    except Exception as e:\n",
    "        runtime, accuracy_val, error_rate,acceptance_rate , invalid_acceptance_rate, remark = (0, 0, 0), 0, 0, 0, 0, e\n",
    "        logger.error(f\"Error during experiment for domain {domain}: {e}\")\n",
    "\n",
    "    polocm_time, locm2_time, locm_time = runtime\n",
    "    logger.info(f\"{domain}-lo.{learning_obj['id']}-{dod}  DONE\")\n",
    "\n",
    "    result_data = {\n",
    "        'id': learning_obj['id'],\n",
    "        'dod': dod,\n",
    "        'actual_dod': actual_dod,\n",
    "        'domain': domain,\n",
    "        'index': learning_obj['index'],\n",
    "        'total_length': learning_obj['total_length'],\n",
    "        'len%': learning_obj['len%'],\n",
    "        'runtime': sum(runtime),\n",
    "        'polocm_time': polocm_time,\n",
    "        'locm2_time': locm2_time,\n",
    "        'locm_time': locm_time,\n",
    "        'accuracy': accuracy_val,\n",
    "        'error_rate': error_rate,\n",
    "        'acceptance_rate': acceptance_rate,\n",
    "        'invalid_acceptance_rate': invalid_acceptance_rate,  # Placeholder for invalid acceptance rate\n",
    "        'remark': remark\n",
    "    }\n",
    "    write_result_to_csv(dod, extraction_type, result_data)\n",
    "    return\n",
    "\n",
    "def write_result_to_csv(dod, extraction_type, result_data):\n",
    "    \"\"\"Writes the result data to a CSV file in a thread-safe manner.\"\"\"\n",
    "    csv_file_path = os.path.join(OUTPUT_DIR, f\"results_{dod}_{extraction_type}.csv\")\n",
    "    file_exists = os.path.exists(csv_file_path)\n",
    "    with open(csv_file_path, 'a') as csv_file:\n",
    "        if not file_exists:\n",
    "            headers = result_data.keys()\n",
    "            csv_file.write(','.join(headers) + '\\n')\n",
    "\n",
    "        values = [str(result_data[key]) for key in result_data.keys()]\n",
    "        csv_file.write(','.join(values) + '\\n')\n",
    "\n",
    "\n",
    "@set_timer_throw_exc(num_seconds=600, exception=GeneralTimeOut, max_time=600)\n",
    "def solve(cplex_dir,cplex_threads, extraction,po_traces ,traces, domain_name, test_data, invalid_test_suffixes):\n",
    "    try: \n",
    "        remark = []\n",
    "        extraction_method = extraction(cplex_dir, cplex_threads)\n",
    "        \n",
    "    \n",
    "        model, TM , runtime = extraction_method.extract_model(po_traces)\n",
    "    \n",
    "        pddl_model = model.to_pddl_domain(domain_name)\n",
    "        golden_TM = extraction_method.get_TM_list(traces)\n",
    "    \n",
    "        accuracy_val,error_rate, r = get_AP_accuracy(TM, golden_TM)\n",
    "        if r:\n",
    "            remark.append(r)\n",
    "        acceptance_rate, invalida_acceptance_rate, r = get_acceptance_rate(pddl_model, test_data, invalid_test_suffixes)\n",
    "        if r:\n",
    "            remark.append(r)\n",
    "\n",
    "        if len(remark)==0:\n",
    "            remark = ['Success']\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        extraction_method.terminate()\n",
    "        return (0,0,0), 0,0, 0,0, e\n",
    "    return runtime, accuracy_val, error_rate, acceptance_rate, invalida_acceptance_rate, \" \".join(remark)\n",
    "\n",
    "\n",
    "def get_AP_accuracy(TM, golden_TM):\n",
    "    if (len(TM)==0):\n",
    "        return 0,1, \"AP Empty\"\n",
    "    if (len(TM) != len(golden_TM)):\n",
    "        return 0,1, \"AP Invalid Length\"\n",
    "    acc = []\n",
    "    fpr = []\n",
    "\n",
    "    def get_golden_TM(TM_cols):\n",
    "        for golden_tm in golden_TM:\n",
    "            if set(golden_tm.columns) == TM_cols:\n",
    "                return golden_tm\n",
    "        return None\n",
    "    for sort, m1 in enumerate(TM):\n",
    "        m1_cols = set(m1.columns)\n",
    "        golden_tm = get_golden_TM(m1_cols)\n",
    "        assert golden_tm is not None, f\"Golden TM for sort {sort}: {m1_cols} not found in golden_TM\"\n",
    "        \n",
    "        m1 = m1.reindex(index=golden_tm.index, columns=golden_tm.columns) \n",
    "        m1 = np.where(m1>0, 1, 0)\n",
    "        \n",
    "        l1 = m1.flatten()\n",
    "\n",
    "        m2 = np.where(golden_tm>0, 1,0)\n",
    "        l2 = m2.flatten()\n",
    "      \n",
    "        # print(f\"sort{sort}-AP array [learned]: {l1}\")\n",
    "        # print(f\"sort{sort}-AP array [ground ]: {l2}\")\n",
    "        acc.append(sum(l1==l2)/len(l1))\n",
    "        fpr.append(np.sum((l2==0)& (l1==1))/len(l1)) # one side error rate\n",
    "        # fp = np.sum((l2==0) & (l1==1))\n",
    "        # tn = np.sum((l2==0) & (l1==0))\n",
    "        # fpr.append(fp / (fp + tn) if (fp + tn) > 0 else 0)\n",
    "        \n",
    "    return sum(acc)/len(acc), sum(fpr)/len(fpr), None\n",
    "\n",
    "\n",
    "def get_acceptance_rate(learned_domain, test_data, invalid_test_suffixes):\n",
    "   \n",
    "    try:\n",
    "        evaluator = ExecutabilityEvaluator(learned_domain)\n",
    "        valid_res = []\n",
    "        invalid_res = []\n",
    "        \n",
    "        for problem, trace in test_data.items():\n",
    "            valid_acceptance, invalid_acceptance = evaluator.get_acceptance_rate(trace, invalid_test_suffixes[problem])\n",
    "            valid_res.append(valid_acceptance)\n",
    "            # Only considert invalid acceptance if the valid seq is accepted\n",
    "            if valid_acceptance == 1:\n",
    "                invalid_res.append(invalid_acceptance)\n",
    "        if len(valid_res) == 0:\n",
    "            valid = 0\n",
    "        else:\n",
    "            valid = sum(valid_res) / len(valid_res)\n",
    "        if len(invalid_res) == 0:\n",
    "            invalid = 0\n",
    "        else:\n",
    "            invalid = sum(invalid_res) / len(invalid_res)\n",
    "    except Exception as e:\n",
    "        return 0,0, \"Error in acceptance rate calculation\" + str(e)\n",
    "    return valid, invalid, None\n",
    "\n",
    "def read_files(input_filepath, test_filepath, invalid_suffixes_filepath):\n",
    "    \n",
    "    \n",
    "    TRAIN_DATA  = read_json_file(input_filepath)\n",
    "\n",
    "    plain_traces = defaultdict(lambda: defaultdict())\n",
    "    with open(test_filepath, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            details = line.split(\"&&\")\n",
    "\n",
    "            domain_name = details[0]\n",
    "            problem_name = details[2]\n",
    "            plan = details[-1]\n",
    "\n",
    "            plain_traces[domain_name][problem_name]= read_plan(plan)\n",
    "    TEST_DATA = plain_traces\n",
    "\n",
    "    invalid_suffixes = defaultdict(lambda: defaultdict(list))\n",
    "\n",
    "    with open(invalid_suffixes_filepath, \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            details = line.split(\"&&\")\n",
    "            domain_name = details[0]\n",
    "            problem_name = details[1]\n",
    "            plan = details[2]\n",
    "            \n",
    "            invalid_suffixes[domain_name][problem_name].append(read_plan(plan)[0])\n",
    "    INVALID_TEST_SUFFIXES = invalid_suffixes\n",
    "    return TRAIN_DATA, TEST_DATA, INVALID_TEST_SUFFIXES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "247dd027",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c6994caf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results_0.1_p2.csv\n",
      "Missing IDs: []\n",
      "results_0.2_p2.csv\n",
      "Missing IDs: []\n",
      "results_0.3_p2.csv\n",
      "Missing IDs: []\n",
      "results_0.4_p2.csv\n",
      "Missing IDs: []\n",
      "results_0.5_p2.csv\n",
      "Missing IDs: []\n",
      "results_0.6_p2.csv\n",
      "Missing IDs: []\n",
      "results_0.7_p2.csv\n",
      "Missing IDs: []\n",
      "results_0.8_p2.csv\n",
      "Missing IDs: []\n",
      "results_0.9_p2.csv\n",
      "Missing IDs: [884]\n",
      "results_1.0_p2.csv\n",
      "Missing IDs: []\n"
     ]
    }
   ],
   "source": [
    "expected_ids = set(range(1,1335))\n",
    "for result_file in os.listdir(INPUT_DIR):\n",
    "    df = pd.read_csv(os.path.join(INPUT_DIR, result_file))\n",
    "    existing_ids = set(df['id'])\n",
    "    missing_ids = sorted(expected_ids - existing_ids)\n",
    "    print(result_file)\n",
    "    print(f\"Missing IDs: {missing_ids}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "656c4251",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"../../data/plain_traces/plain_traces.txt\", \"r\") as f:\n",
    "#     lines = f.readlines()\n",
    "#     for line in lines:\n",
    "#         details = line.split(\"&&\")\n",
    "#         name = f\"{details[0]}-{details[2]}-{details[3]}\"\n",
    "      \n",
    "#         plan = details[-1]\n",
    "#         trace = read_plan(plan)\n",
    "#         po_trace = trace.to_partial_ordered_trace(0.3)\n",
    "#         test_exe(name, po_trace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c0a9d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2306529e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc5da96",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
